{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6492477,"sourceType":"datasetVersion","datasetId":3752087}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T08:48:52.715068Z","iopub.execute_input":"2025-08-21T08:48:52.715590Z","iopub.status.idle":"2025-08-21T08:48:52.722679Z","shell.execute_reply.started":"2025-08-21T08:48:52.715564Z","shell.execute_reply":"2025-08-21T08:48:52.721981Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bbc-news/bbc-text.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# DATASET ","metadata":{}},{"cell_type":"markdown","source":"\n**You should include the BBC News dataset using the input tab on the right side.  I used this dataset**\n: [https://www.kaggle.com/datasets/moazeldsokyx/bbc-news](https://www.kaggle.com/datasets/moazeldsokyx/bbc-news)\n","metadata":{}},{"cell_type":"markdown","source":"# model","metadata":{}},{"cell_type":"code","source":"# import tensorflow_hub as hub\n# import bert.tokenization.FullTokenizer as tokenizer\n\n# I'd like to use TensorFlow libraries here,\n# but my internet speed is only around 0.3 Mbps,\n# which means it takes over an hour to download both TensorFlow Hub and the tokenizer.\n# So, for now, I'm skipping them.\n\nfrom transformers import TFAutoModel, AutoTokenizer\nimport tensorflow as tf\nimport keras\n\nmodel_name = \"bert-base-uncased\"\nbert_layer = TFAutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nclass TextClassifier(keras.Model):\n    def __init__(self, bert_layer, num_classes):\n        super(TextClassifier, self).__init__()\n        self.bert = bert_layer\n        self.dropout = keras.layers.Dropout(0.3)\n        self.dense = keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        outputs = self.bert(inputs)\n        sequence_output = outputs.last_hidden_state\n        cls_token = sequence_output[:, 0, :] \n        x = self.dropout(cls_token)\n        return self.dense(x)\n\n\nmodel = TextClassifier(bert_layer, num_classes=5)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\n# ----------------------------------\n# reding data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\n\n\ndf = pd.read_csv(\"/kaggle/input/bbc-news/bbc-text.csv\")\nprint(df.info())\nprint(df.head())\n\nlabel2id = {label: idx for idx, label in enumerate(sorted(df[\"category\"].unique()))}\ndf[\"label_id\"] = df[\"category\"].map(label2id)\n\ntrain_df, val_df = train_test_split(\n    df, test_size=0.1, stratify=df[\"label_id\"], random_state=42\n)\n\n\ndef preprocessing(texts, labels, tokenizer, max_length=128):\n    enc = tokenizer(\n        texts.tolist(),\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        return_tensors=\"tf\"\n    )\n    inputs = {\n        \"input_ids\": enc[\"input_ids\"],\n        \"attention_mask\": enc[\"attention_mask\"],\n        \"token_type_ids\": enc[\"token_type_ids\"],\n    }\n    return inputs, tf.convert_to_tensor(labels.tolist())\n\n\ntrain_enc, train_labels = preprocessing(\n    train_df[\"text\"], train_df[\"label_id\"], tokenizer\n)\nval_enc, val_labels = preprocessing(\n    val_df[\"text\"], val_df[\"label_id\"], tokenizer\n)\n\ntrain_ds = (\n    tf.data.Dataset.from_tensor_slices((train_enc, train_labels))\n    .shuffle(500)\n    .batch(16)\n)\nval_ds = tf.data.Dataset.from_tensor_slices((val_enc, val_labels)).batch(16)\n\n\nmodel.fit(train_ds, validation_data=val_ds, epochs=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T08:48:52.723925Z","iopub.execute_input":"2025-08-21T08:48:52.724155Z","iopub.status.idle":"2025-08-21T08:59:41.866143Z","shell.execute_reply.started":"2025-08-21T08:48:52.724140Z","shell.execute_reply":"2025-08-21T08:59:41.865508Z"}},"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2225 entries, 0 to 2224\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   category  2225 non-null   object\n 1   text      2225 non-null   object\ndtypes: object(2)\nmemory usage: 34.9+ KB\nNone\n        category                                               text\n0           tech  tv future in the hands of viewers with home th...\n1       business  worldcom boss  left books alone  former worldc...\n2          sport  tigers wary of farrell  gamble  leicester say ...\n3          sport  yeading face newcastle in fa cup premiership s...\n4  entertainment  ocean s twelve raids box office ocean s twelve...\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1755766157.444155     110 assert_op.cc:38] Ignoring Assert operator text_classifier_6_1/tf_bert_model_6/bert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m125/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.2938 - loss: 1.5557","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1755766179.309161     111 assert_op.cc:38] Ignoring Assert operator text_classifier_6_1/tf_bert_model_6/bert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.2944 - loss: 1.5550","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1755766186.380563     109 assert_op.cc:38] Ignoring Assert operator text_classifier_6_1/tf_bert_model_6/bert/embeddings/assert_less/Assert/Assert\nW0000 00:00:1755766190.452233     111 assert_op.cc:38] Ignoring Assert operator text_classifier_6_1/tf_bert_model_6/bert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 258ms/step - accuracy: 0.2950 - loss: 1.5543 - val_accuracy: 0.5516 - val_loss: 1.3203\nEpoch 2/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - accuracy: 0.5707 - loss: 1.2897 - val_accuracy: 0.7220 - val_loss: 1.1467\nEpoch 3/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.7256 - loss: 1.1201 - val_accuracy: 0.8161 - val_loss: 1.0059\nEpoch 4/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 157ms/step - accuracy: 0.8098 - loss: 0.9798 - val_accuracy: 0.8565 - val_loss: 0.8898\nEpoch 5/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 160ms/step - accuracy: 0.8556 - loss: 0.8628 - val_accuracy: 0.9013 - val_loss: 0.7946\nEpoch 6/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 164ms/step - accuracy: 0.8972 - loss: 0.7614 - val_accuracy: 0.9103 - val_loss: 0.7151\nEpoch 7/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.8977 - loss: 0.6988 - val_accuracy: 0.9238 - val_loss: 0.6483\nEpoch 8/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.9112 - loss: 0.6245 - val_accuracy: 0.9417 - val_loss: 0.5920\nEpoch 9/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9251 - loss: 0.5672 - val_accuracy: 0.9462 - val_loss: 0.5439\nEpoch 10/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 160ms/step - accuracy: 0.9254 - loss: 0.5290 - val_accuracy: 0.9507 - val_loss: 0.5028\nEpoch 11/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 160ms/step - accuracy: 0.9226 - loss: 0.4934 - val_accuracy: 0.9552 - val_loss: 0.4680\nEpoch 12/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 160ms/step - accuracy: 0.9374 - loss: 0.4531 - val_accuracy: 0.9641 - val_loss: 0.4375\nEpoch 13/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9301 - loss: 0.4227 - val_accuracy: 0.9641 - val_loss: 0.4111\nEpoch 14/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9393 - loss: 0.4014 - val_accuracy: 0.9686 - val_loss: 0.3875\nEpoch 15/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9344 - loss: 0.3783 - val_accuracy: 0.9686 - val_loss: 0.3658\nEpoch 16/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9432 - loss: 0.3517 - val_accuracy: 0.9686 - val_loss: 0.3473\nEpoch 17/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9494 - loss: 0.3251 - val_accuracy: 0.9686 - val_loss: 0.3306\nEpoch 18/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9407 - loss: 0.3268 - val_accuracy: 0.9686 - val_loss: 0.3158\nEpoch 19/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9446 - loss: 0.3075 - val_accuracy: 0.9686 - val_loss: 0.3010\nEpoch 20/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9518 - loss: 0.2907 - val_accuracy: 0.9686 - val_loss: 0.2890\nEpoch 21/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9510 - loss: 0.2882 - val_accuracy: 0.9686 - val_loss: 0.2773\nEpoch 22/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9543 - loss: 0.2619 - val_accuracy: 0.9641 - val_loss: 0.2668\nEpoch 23/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9535 - loss: 0.2621 - val_accuracy: 0.9731 - val_loss: 0.2563\nEpoch 24/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9557 - loss: 0.2474 - val_accuracy: 0.9731 - val_loss: 0.2476\nEpoch 25/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9567 - loss: 0.2437 - val_accuracy: 0.9731 - val_loss: 0.2389\nEpoch 26/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9592 - loss: 0.2333 - val_accuracy: 0.9731 - val_loss: 0.2308\nEpoch 27/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9556 - loss: 0.2335 - val_accuracy: 0.9731 - val_loss: 0.2245\nEpoch 28/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9611 - loss: 0.2137 - val_accuracy: 0.9731 - val_loss: 0.2171\nEpoch 29/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9539 - loss: 0.2212 - val_accuracy: 0.9731 - val_loss: 0.2108\nEpoch 30/30\n\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9608 - loss: 0.2046 - val_accuracy: 0.9731 - val_loss: 0.2047\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x783ebdfcab50>"},"metadata":{}}],"execution_count":13}]}