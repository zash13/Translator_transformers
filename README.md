# Recurrent Neural Networks (RNN) and Transformers â€“ Resource Guide

## RNN Basics

- [Introduction to Recurrent Neural Networks](https://www.geeksforgeeks.org/machine-learning/introduction-to-recurrent-neural-network/)
- [Backpropagation Through Time in RNNs](https://www.geeksforgeeks.org/machine-learning/ml-back-propagation-through-time/)
- [Vanishing and Exploding Gradients in Deep Learning](https://www.geeksforgeeks.org/deep-learning/vanishing-and-exploding-gradients-problems-in-deep-learning)

## LSTM (Long Short-Term Memory)

- [What is LSTM?](https://www.youtube.com/watch?v=b61DPVFX03I)
- [LSTM Explained Clearly](https://www.youtube.com/watch?v=YCzL96nL7j0)
- [LSTM RNN Overview](https://www.youtube.com/watch?v=wgfSDrqYMJ4)

## Comparative Architectures

- [RNN vs LSTM vs GRU vs Transformers](https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/)

## Transformers

- [Introduction to Transformers in Machine Learning](https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/)
- [Text Classification with Transformers (Keras)](https://keras.io/examples/nlp/text_classification_with_transformer/)
- [Self-Attention vs Attention in Transformers](https://medium.com/@wwydmanski/whats-the-difference-between-self-attention-and-attention-in-transformer-architecture-3780404382f3)
- [BERT paper](https://arxiv.org/pdf/1810.04805)

- [bert Embeddings how it work ](https://tinkerd.net/blog/machine-learning/bert-embeddings/)
- [bert fine tuning](https://reintech.io/blog/fine-tuning-bert-text-classification-tensorflow?utm_source=chatgpt.com)

## DATASET

- [Quora Question Pairs - kaggle ](https://www.kaggle.com/competitions/quora-question-pairs/data)
- [2,225 news articles published by BBC News - kaggle ](https://www.kaggle.com/datasets/moazeldsokyx/bbc-news/data)
