# Recurrent Neural Networks (RNN) and Transformers â€“ Resource Guide

## RNN Basics

- [Introduction to Recurrent Neural Networks](https://www.geeksforgeeks.org/machine-learning/introduction-to-recurrent-neural-network/)
- [Backpropagation Through Time in RNNs](https://www.geeksforgeeks.org/machine-learning/ml-back-propagation-through-time/)
- [Vanishing and Exploding Gradients in Deep Learning](https://www.geeksforgeeks.org/deep-learning/vanishing-and-exploding-gradients-problems-in-deep-learning)

## LSTM (Long Short-Term Memory)

- [What is LSTM?](https://www.youtube.com/watch?v=b61DPVFX03I)
- [LSTM Explained Clearly](https://www.youtube.com/watch?v=YCzL96nL7j0)
- [LSTM RNN Overview](https://www.youtube.com/watch?v=wgfSDrqYMJ4)

## Comparative Architectures

- [RNN vs LSTM vs GRU vs Transformers](https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/)

## Transformers

- [Introduction to Transformers in Machine Learning](https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/)
- [Text Classification with Transformers (Keras)](https://keras.io/examples/nlp/text_classification_with_transformer/)
- [Self-Attention vs Attention in Transformers](https://medium.com/@wwydmanski/whats-the-difference-between-self-attention-and-attention-in-transformer-architecture-3780404382f3)
